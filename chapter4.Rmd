# Clustering and classification

First thing is to load required libraries or R packages for this exercise. Also set random seed for repeatability of the results.

```{r packages4, warning=FALSE, message=FALSE, echo=TRUE}
# Required libraries
library(MASS)
library(corrplot)
library(tidyverse)
library(plotly)
set.seed(12345)

```

## Read the data

'Boston' data from 'MASS' package is read here. The data is about housing values and factor affecting housing values in Suburbs of Boston. The data has 506 rows and 14 columns.
Each row has 14 aspect of the suburb line residential area, business area, pupil-teacher ratio, taxes etc.
Let's explore the data in more details.


```{r data}

# load the data
data("Boston")

# Structure of the dataset 

str(Boston)

# Dimension of the dataset
dim(Boston)

```

Summaries and distributions of the variables.

```{r pairs, echo=TRUE, warning=FALSE, message=FALSE,fig.height = 10, fig.width = 10, fig.align = "center"}

# Summary the dataset
summary(Boston)

# distribution of variables and their relationships with each other
pairs(Boston)

# Better graphical presentation with ggpairs()
p <- ggpairs(Boston, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))

# draw the plot
p


```

Distribution of all the variables can be seen from the above plot. 
'rm' i.e Average number of rooms per dwelling has a nice bell shape curve suggesting normal distribution with mean 6.208. Variables 'dis', 'Istat' and 'medv' have Normal distribution curve with longer right tail. 'indus' and 'tax' seems bimodal.

To explore the relationships between the variables let's use 'corrplot'

```{r corr, echo=FALSE, warning=FALSE, message=FALSE,fig.height = 10, fig.width = 10, fig.align = "center"}

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

p
```

'medv'(median value of owner-occupied homes in \$1000s.) shows correlation with all the variables.
Except variable 'chas' (binary variable 'tract bounds river'), all other variables have good correlation (positive or negative) with each other.

From the colorful corrplot, it can be seen that the highest positive correlation is between 'tax' (full-value property-tax rate per \$10,000.) and 'rad' (index of accessibility to radial highways).

dis (weighted mean of distances to five Boston employment centres) has strong negative correlation with indus(proportion of non-retail business acres per town), age(proportion of owner-occupied units built prior to 1940) and nox(nitrogen oxides concentration (parts per 10 million)) indicating that near the employment centres there is higher proportion of non-retail business, higher proportion of owner-occupied units built prior to 1940 and higher  nitrogen oxides concentration.
As expected, Istat(lower status of the population (percent)) and medv(median value of owner-occupied homes in \$1000s) are also highely negatively correlated.

'chas' (binary variable 'tract bounds river') has hardly any correlation with other variables. It is clear that although in the human history, human beings chose to stay near rivers or water sources in the ancient days, today it has less impact on choosing a housing.  

```{r scale, echo=TRUE}

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)
```

After scaling, all the variable ranges have reduced. Most of the 1st and 3rd quantiles are between (or near) -1 and 1. 
Next step is creating categorical variable of crime with each category indicating level of crime. Then the original variable 'crim' will be removed and this categorical variable will be added as a new column.

```{r crime categorial, echo=TRUE, warning=FALSE, message=FALSE}

# create data frame object
boston_scaled <- as.data.frame(boston_scaled)

# create a categorical variable 'crime' with cut points as quantiles of the variable 'crim'
crime <- cut(boston_scaled$crim, breaks = quantile(boston_scaled$crim), include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Let's create train data by randomly selecting 80% of the rows of the data. Remaining rows will be treated as test data. 

```{r train test data, echo=TRUE, warning=FALSE, message=FALSE}

# choose randomly 80% of the rows for training set
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]



```

## Linear discriminant analysis (LDA)

Linear discriminant analysis is performed on train data using crime as target variable and all other variables as covariates.After LDA fit is obtained the biplot is plotted.

```{r lda, warning=FALSE, message=FALSE,fig.height = 10, fig.width = 10, fig.align = "center"}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

'rad' has a longest arrow so it is the most discriminant among all the predictors. 'rad' can differentiate between crime categories well.

## LDA prediction

We would like to predict 'crime' categories for the test data so keep a copy of true 'crime' categories of the test data in 'correct_classes' and then delete 'crime' column from test data.

```{r lda predict, echo=TRUE}

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

The cross tabulation show that category 'high' in the test data is very well predicted. Category 'med_high' shows worst categorization prediction. Out of 30 'med_high category in the test data, only 16 are correctly predicted. 'low' and 'med_low' categories are predicted 2/3 times correctly. 

## K-means clustering

Let's consider the Boston data again. Compute various distances like euclidean distance, manhattan distance on the scaled Boston data. 

```{r edu}

data("Boston")
# center and standardize variables
boston_scaled <- scale(Boston)


# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of euclidean distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method = 'manhattan')

# look at the summary of manhattan distances
summary(dist_man)
```
Even on the scaled data, two different distances have very different range.
Let's perform k-means clustering with 3 clusters.

```{r k-means, echo=TRUE, warning=FALSE, message=FALSE,fig.height = 10, fig.width = 10, fig.align = "center"}

# k-means clustering
km3 <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km3$cluster)


```

Overall, the graphs show groups of 3 colors but all 3 colors are not seen in all the graphs. Also, some times they do not properly form clusters.

To find appropriate number of clusters, consider k= 1 to 10 all 10 values one by one.

```{r k-means 1-10, echo=TRUE, warning=FALSE, message=FALSE,fig.height = 5, fig.width = 5, fig.align = "center"}

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

The plot shows that from cluster 1 to 2 there is large improvement. There is not much further improvement. If we are interested in very small SS (sum of squares) then one can consider more clusters. However, having too many clusters is not always useful if they do not actually differ in all the variables. Let's go ahead with k =2 which has good improvement over k=1.

```{r k-means 2, echo=TRUE, warning=FALSE, message=FALSE,fig.height = 10, fig.width = 10, fig.align = "center"}


# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```

Almost all the smaller graphs show good separation of 2 groups with 2 colors. Hence, k=2 was a good choice.


## LDA on k-means clusters

Let's use k=4 to generate clusters. Then perform LDA to check how these 4 clusters are analyzed using linear discrimination analysis.

```{r k-means LDA, echo=TRUE, warning=FALSE, message=FALSE,fig.height = 10, fig.width = 10, fig.align = "center"}

# k-means clustering
km4 <-kmeans(boston_scaled, centers = 4)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km4$cluster)


boston_scaled1 = data.frame(boston_scaled)
boston_scaled1= boston_scaled1 %>%
  mutate(cluster =km4$cluster )

# linear discriminant analysis
lda.fit4 <- lda(cluster ~ ., data = boston_scaled1)

# print the lda.fit object
lda.fit4


# target classes as numeric
classes <- as.numeric(boston_scaled1$cluster)

# plot the lda results
plot(lda.fit4, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit4, myscale = 2)

```

Variables with longer arrows in the plot are the best linear discriminants for these 4 groups. 

## Visualization using plotly

```{r k-means plotly, echo=TRUE, warning=FALSE, message=FALSE,fig.height = 7, fig.width = 7, fig.align = "center"}

model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color= train$crime)


```